\documentclass{beamer}
\usetheme{Copenhagen}
\usepackage[orientation=landscape,size=a4,scale=0.7]{beamerposter}
\usepackage{array}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{physics}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{ccicons}

\DeclareUnicodeCharacter{2005}{\hspace{0em}}

\title{Statistics cheatsheet - MITx Micromaster in Statistics and Data Science}
\author{Alejandro Fernández Camello}
\date{\vspace{-100pt}}

\setbeamertemplate{navigation symbols}{}
\usecolortheme[named=red]{structure}

\begin{document}
\begin{frame}

\maketitle

\begin{columns}

\begin{column}{0.3\textwidth}
\begin{block}{Statistical model}

Let the observed outcome of a statistical experiment be a sample $X1, . . . , Xn$ of $n$ i.i.d. random variables in some measurable space $E$ and denote by IP their common distribution. A statistical model associated to that statistical experiment is a pair

\begin{align*}
    (E, (\mathbb{P}_\theta)\theta \in \Theta)
\end{align*}

where:

\begin{itemize}
    \item $E$ is the sample space
    \item $(\mathbb{P}_\theta)\theta \in \Theta$ is a family of probability measures on $E$
    \item $\Theta$ is the parameter set
\end{itemize}

If the number of parameters is finite, it's called a parametric statistical model. Otherwise, it can be semiparametric (some part parametric and the other nonparametric) and nonparametric (doesn't assume anything about the underlying distribution).
    
\end{block}

\begin{block}{Estimation}

An estimator of $\theta$ is any statistic (any measurable function of the sample) that doesn't depend of $\theta$. An estimator $\hat{\theta}_n$ of $theta$ is asymptotically normal if

\begin{align*}
    \sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow[n \to \infty]{(d)} \mathcal{N}(0, \sigma^2)
\end{align*}

The bias of an estimator is (if the bias is 0 then the estimator is unbiased):

\begin{align*}
    \text{bias}(\hat{\theta}_n) = \mathbb{E}[\hat{\theta}_n] - \theta
\end{align*}

The variance of an estimator is defined in the same way as any random variable. The quadratic risk is defined as bias² + variance. The goal is to minimize it to have a good estimator. The two principal methods of estimation are MLE (Maximum Likelihood Estimator) and moments. The MLE is the parameter/parameters that maximized the likelihood function for a sample $x_1, \ldots, x_n$ defined in the following way:

\begin{align*}
    L(x_1, \ldots, x_n; \theta) = \prod_{i=1}^n P_\theta(x_i)
\end{align*}

To find the maximum of the function is usually take the log of the previous function to simplify the calculations. The Fisher information is defined as:

\begin{align*}
    I(\theta) = Var(\ell'(\theta)) = -\mathbb{E}[\ell''(\theta)]
\end{align*}

Under some general conditions the MLE follows the following conditions:

\begin{align*}
    \hat{\theta}_n \xrightarrow[n \to \infty]{P} \theta^* \text{ w.r.t } P_{\theta^*}
\end{align*}

\begin{align*}
    \sqrt{n}(\hat{\theta}_n - \theta^*) \xrightarrow[n \to \infty]{(d)} \mathcal{N}(0, I(\theta^*)^{-1}) \text{ w.r.t } P_{\theta^*}
\end{align*}

For the methods of moments, the $k$-moments are used for computing the parameter or parameters of the model. The $k$-moment ($m_k$) is defined as: 

\begin{align*}
    m_k = \mathbb{E}[X^k]
\end{align*}

To calculate moments, the Moment Generating Function is used, which is defined as $M_X(t) = \mathbb{E}[e^{tX}]$, where $t$ is a real constant. This function provides the $k^{th}$ moment $m_k$ when differentiated $k$ times with respect to $t$ and evaluated at $t=0$. Once the moments are computed, relationships between these moments and the distribution parameters can be established to ultimately calculate the parameters.

\end{block}

\end{column}

\begin{column}{0.3\textwidth}

\begin{block}{Limit theorems}

\textbf{Markov inequality}

If a random variable $X$ cannot take negative values, then

\begin{align*}
    P(X \geq a) \leq \frac{\mathbb{E}[X]}{a}, \forall a > 0
\end{align*}

\textbf{Chebyshev inequality}

If a random variable $X$ with mean $\mu$ and variance $\sigma$, then

\begin{align*}
    P(|X - \mu| \geq c) \leq \frac{\sigma^2}{c^2}, \forall c > 0
\end{align*}

\textbf{The weak law of large numbers}

Let $X_i$ be $n$ i.i.d random variables with mean $\mu$. For every $\epsilon > 0$, we have:

\begin{align*}
    P\left(\abs{\frac{\sum_i^n X_i}{n} - \mu} \geq \epsilon\right) \to 0, \text{as }  n \to \infty
\end{align*}

\textbf{The strong law of large numbers}

Let $X_i$ be $n$ i.i.d random variables with mean $\mu$, we have:

\begin{align*}
    P\left( \lim_{n \to \infty} \frac{\sum_i^n X_i}{n} = \mu\right) = 1
\end{align*}

\textbf{Central limit theorem}

Let $X_1, X_2, \ldots$ be a sequence of i.i.d. random variables with mean $\mu$ and finite variance $\sigma^2$. Then:

\begin{align*}
    \lim_{n \to \infty} \frac{1}{\sigma\sqrt{n}} \sum_{i=1}^{n} \left( X_i - \mu \right) \xrightarrow{(d)} \mathcal{N}(0, 1).
\end{align*}

\end{block}

\begin{block}{Convergence theorems}

\textbf{Almost surely convergence}

A sequence of random variables $X_1, X_2, \ldots$ converges almost surely (a.s.) to $X$ if 

\begin{align*}
  P\left( \lim_{n \to \infty} X_n = X \right) = 1  
\end{align*}

\textbf{Convergence in probability}

A sequence of random variables $X_1, X_2, \ldots$ converges in probability to \(X\) if for every $\epsilon > 0$,

\begin{align*}
   \lim_{n \to \infty} P(|X_n - X| > \epsilon) = 0 
\end{align*}

\textbf{Convergence in distribution}

A sequence of random variables $X_1, X_2, \ldots$ converges in distribution to $X$ if 

\begin{align*}
    \lim_{n \to \infty} F_{X_n}(x) = F_X(x)
\end{align*}

for all $x$ at which \(F_X(x)\) is continuous, where $F_{X_n}(x)$and $F_X(x)$ are the cumulative distribution functions of $X_n$ and $X$, respectively.

\end{block}

\end{column}

\begin{column}{0.3\textwidth}

\begin{block}{Confidence Interval}

A confidence interval of level \(1-\alpha\) for a parameter \(\theta\) is defined as a random interval \(\mathcal{I}\), the boundaries of which are independent of \(\theta\). This interval satisfies:

\begin{align*}
    P_\theta\left(\theta \in \mathcal{I}\right) \geq 1 - \alpha, \quad \forall \theta \in \Theta
\end{align*}

The interval is said to be asymptotic if it also satisfies:

\begin{align*}
    \lim_{n \to \infty} P_\theta\left(\theta \in \mathcal{I}\right) \geq 1 - \alpha, \quad \forall \theta \in \Theta
\end{align*}

When estimating a mean, the confidence interval (\(CI\)) can be calculated as follows:

\begin{align*}
    CI = \left( \bar{x} - q_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + q_{\alpha/2} \frac{\sigma}{\sqrt{n}} \right)
\end{align*}

Here, \(\bar{x}\) is the sample mean, \(q_{\alpha/2}\) is the \((1 - \alpha/2)\)-th quantile of the normal distribution, \(\sigma\) is the standard deviation (calculated in one of three ways: conservative, solve, or plug-in), and \(n\) is the sample size. The Delta method enables the calculation of confidence intervals for parameters that are functions \(g(x)\) of the mean. The function \(g(x)\) must be continuously differentiable at the point \(\theta\) and asymptotically normal. The Delta method yields the following result:

\begin{align*}
    \sqrt{n}(g(\bar{x}) - g(\theta)) \xrightarrow[n \to \infty]{(d)} \mathcal{N}(0, (g'(\theta))^2 \sigma^2)
\end{align*}

\end{block}

\begin{block}{Hypothesis Testing}

Parametric hypothesis testing aims to provide a binary (yes/no) answer regarding whether a specific parameter or set of parameters lies within a certain interval. The process involves formulating two hypotheses: the null hypothesis, denoted as \(H_0\), and the alternative hypothesis, denoted as \(H_1\). The objective is to evaluate \(H_0\) in the context of \(H_1\). There are two possible outcomes: either \(H_0\) is retained, representing the status quo, or it is rejected, signaling a "discovery." Each hypothesis corresponds to a specific parameter space or set of statistical models.

In hypothesis testing, two types of errors can occur:

\begin{itemize}
    \item Type I error: Occurs when we reject \(H_0\) even though it is true.
    \item Type II error: Occurs when we fail to reject \(H_0\) even though it is false.
\end{itemize}

The power function, defined as follows, allows for the computation of both errors:

\begin{align*}
    \beta(\theta) = P_\theta[\psi = 1]
\end{align*}

Here, \(\psi\) is the test statistic that can take the value 0 or 1. A value of 0 implies that \(H_0\) is not rejected, and a value of 1 implies that \(H_0\) is rejected. Under the Neyman-Pearson paradigm, the aim is to keep the Type I error rate at or below a specified level \(\alpha\), while minimizing the Type II error given this constraint. The (asymptotic) p-value of a test \(\psi\) is the smallest level \(\alpha\) at which \(\psi\) rejects \(H_0\). The p-value serves as a metric for evaluating the evidence against \(H_0\) and determining whether it should be rejected. The ranges of p-values commonly used for this purpose are:

\begin{itemize}
    \item \(>0.1\): Almost no evidence
    \item \([0.05, 0.1]\): Weak evidence
    \item \([0.05, 0.01]\): Strong evidence
    \item \([0.01, 0.001]\): Very strong evidence
    \item \(<0.001\): Indisputable evidence
\end{itemize}

\end{block}

\end{column}

\end{columns}

\end{frame}

\begin{columns}
\begin{column}{0.3\textwidth}

\begin{block}{Parametric Hypothesis Testing}

Various tests can be employed depending on the situation. The most widely used are the Wald test, the likelihood ratio test, and the t-test. When calculating p-values, two types of tests are possible: one-sided and two-sided. In the case of a two-sided test, the obtained p-value should be halved. The remainder of the procedure is the same for both types of tests.

\textbf{Wald Test}: Applicable for large samples and based on confidence intervals. The test statistic is given by:

\begin{align*}
    W = \frac{\hat{\theta} - \theta_0}{\sqrt{\hat{\text{var}}(\hat{\theta})}}
\end{align*}

Here, \(\hat{\theta}\) is the estimated parameter, \(\hat{\text{var}}(\hat{\theta}) = \sigma^2/n\), \(\sigma^2\) is the variance according to the parameter estimated, \(n\) is the sample size, and \(\theta_0\) is the null hypothesis value. The test statistic follows a standard normal distribution.

\textbf{Likelihood Ratio Test}: Compares the likelihoods under the null hypothesis and the maximum likelihood estimate (MLE). It is defined as:

\begin{align*}
    T = 2(\ln{L_1} - \ln{L_0})
\end{align*}

Here, \(L_1\) and \(L_0\) are the likelihoods under the MLE and the null hypothesis, respectively. The test statistic follows a \(\chi^2_k\) distribution, where \(k\) is the difference in the number of parameters between the MLE and the null hypothesis.

\textbf{T-test}: Used for small samples (\(n < 30\)) and assumes a normal distribution. The test statistic is:

\begin{align*}
    T = \sqrt{n} \frac{\hat{\mu} - \mu_0}{\sqrt{S^2_n}}
\end{align*}

Here, \(\hat{\mu}\) is the sample mean, \(\mu_0\) is the null hypothesis value, \(S^2_n\) is the unbiased variance estimator, and \(n\) is the sample size. The test statistic follows a \(t_{n-1}\) distribution.

When performing multiple hypothesis tests, care must be taken to avoid false rejections of the null hypothesis. With a significance level \(\alpha\), on average \(n \times \alpha\) errors will occur. Two common corrections are the Bonferroni method and the Benjamini-Hochberg method.

\textbf{Bonferroni Correction}: Adjusts the threshold by dividing \(\alpha\) by the number of tests. This maintains the original Type I error rate but is considered conservative.

\textbf{Benjamini-Hochberg Method}: A more balanced approach, consisting of the following steps:

\begin{enumerate}
    \item Sort the p-values in ascending order.
    \item Find the largest p-value that satisfies \(i \frac{\alpha}{n}\), where \(i\) is the rank, \(\alpha\) is the original significance level, and \(n\) is the number of tests.
    \item Reject null hypotheses with p-values lower than or equal to this value.
\end{enumerate}

\end{block}

\begin{block}{License}
This work is licensed under a \href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International}. \ccbyncsa
\end{block}

\begin{block}{References}
    \begin{itemize}
        \item Wasserman, Larry. \textit{All of statistics: a concise course in statistical inference.} Vol. 26. New York: Springer, 2004.
        \item Notes of \textit{18.650 - Fundamentals of Statistics} offer by MITx in the year 2023.
    \end{itemize}
\end{block}

\end{column}
\begin{column}{0.3\textwidth}

\begin{block}{Nonparametric Hypothesis Testing}

For discrete distributions, the chi-square test ($\chi^2$ test) is commonly used. The test statistic for this method is:

\begin{align*}
    T = n \sum_{j=1}^{k} \frac{(\hat{p}_j - p^0_j)^2}{p^0_j}
\end{align*}

Where $\hat{p}_j$ is the Maximum Likelihood Estimator (MLE) and $p^0_j$ is the null hypothesis value for the $j$-th class. This test follows a $\chi^2_{k-1}$ distribution.

For continuous distributions, the situation is more complex. Goodness-of-fit tests like the Kolmogorov-Smirnov and Kolmogorov-Lilliefors tests are often used.

\textbf{Kolmogorov-Smirnov Test}: This test assesses whether a sample follows a specific distribution by comparing the Cumulative Distribution Functions (CDFs). The test statistic is:

\begin{align*}
    T = \max_{i=1}^{n} \left\{ \max \left( \left| \frac{i-1}{n} - F^0(X_i) \right|, \left| \frac{i}{n} - F^0(X_i) \right| \right) \right\}
\end{align*}

Here, $X_i$ represents the sample values sorted in ascending order, and $F^0$ is the CDF of the distribution under scrutiny. To reject the null hypothesis, the calculated test statistic must exceed the critical value from the Kolmogorov-Smirnov table. 

\textbf{Kolmogorov-Lilliefors Test}: Similar to the Kolmogorov-Smirnov test but allows parameter estimation from the data. The test procedure is the same, but the critical values are obtained from the Kolmogorov-Lilliefors table.

\textbf{QQ-Plots}: Quantile-Quantile plots are popular tools for visual goodness-of-fit assessments. These plots map the CDFs of the null hypothesis and the empirical data onto a two-dimensional space. If the data follows the null hypothesis distribution, the plot should approximately represent the line $y=x$. QQ-plots can also be used to visually compare the heaviness of tails between distributions.

\end{block}

\begin{block}{Linear Regression}

Linear regression models the relationship between two random variables \(X\) and \(Y\) using the regression function \(f(x)\) defined as \(y = a + bx\). The optimal coefficients \(a\) and \(b\) that minimize the mean square loss are given by:

\begin{align*}
    b &= \frac{\text{cov}(X, Y)}{\text{var}(X)} \\
    a &= \mathbb{E}[Y] - b \mathbb{E}[X]
\end{align*}

In practice, we rarely get an exact fit, resulting in a random error term \(\epsilon = Y - (a + bX)\). This error must satisfy:

\begin{align*}
    \mathbb{E}[\epsilon] &= 0 \\
    \text{cov}(X, \epsilon) &= 0
\end{align*}

For multi-dimensional \(X\) (often with an additional dimension of ones to accommodate the intercept \(a\)), the estimator \(\beta\) can be computed as:

\begin{align*}
    \beta = (X^T X)^{-1} X^T Y
\end{align*}

To conduct inference, certain conditions must be met: \(X\) must be deterministic, its rank (\(p\)) must equal the length of the estimator, the errors should be i.i.d, and the noise vector should be Gaussian with constant variance and no correlation between errors. The distribution of the estimator \(\beta\) is given by:

\begin{align*}
    \beta \sim \mathcal{N}\left(B^*, \sigma^2 (X^T X)^{-1}\right)
\end{align*}

The unbiased estimator of \(\sigma^2\) is:

\begin{align*}
    \sigma^2 = \frac{1}{n-p} \lVert Y - X\beta \rVert_2^2
\end{align*}

\end{block}

\end{column}
\begin{column}{0.3\textwidth}

\begin{block}{Bayesian Inference}

Bayesian inference adopts a different approach compared to classical (frequentist) statistics. In Bayesian inference, the parameters being estimated are treated as random variables rather than constants. This allows us to express a belief that the parameters are more likely to be in certain ranges. A probability distribution called the "prior" is chosen to represent this belief. Upon collecting data, the likelihood of the data is combined with the prior to optimize a function using either Maximum A Posteriori (MAP) or the posterior mean. MAP is similar to Maximum Likelihood Estimation (MLE) but incorporates the prior, while the posterior mean calculates the mean of the posterior distribution.

In many cases, we lack specific information about the parameter. For such situations, non-informative priors are often used. A commonly used non-informative prior is the constant prior, which defines a uniform distribution. For unbounded parameters, this is considered an improper prior because it does not fulfill the requirement that the total probability sums to 1. Despite this, it can still be used. Another commonly used non-informative prior is Jeffreys' prior, defined as:

\begin{align*}
    \pi_J(\theta) \propto \sqrt{\det I(\theta)}
\end{align*}

Here, \(I(\theta)\) is the Fisher Information of the parameter \(\theta\). The main advantage of Jeffreys' prior is that it is invariant under reparametrization.

In Bayesian settings, the concept of confidence intervals changes slightly. Bayesian confidence regions are defined as:

\begin{align*}
    P[\theta \in R| X_1, \ldots, X_n] = 1 - \alpha
\end{align*}

This implies that the parameter \(\theta\) has a probability of \(1 - \alpha\) of lying in the subregion \(R\). This is a claim that cannot be made in frequentist confidence intervals, where the parameter, being a constant, is either in or out of the confidence interval, with no associated probability.

\end{block}

\begin{block}{Generalized Linear Models (GLMs)}

Generalized Linear Models (GLMs) extend the concept of linear regression in two key ways:

\begin{align*}
    Y | X = x &\sim \text{some probability distribution}  
\end{align*}

or 

\begin{align*}
    g(\mu(x)) &= x^T \beta
\end{align*}

Here, \(\mu(x)\) is the expected value of \(Y\) conditioned on \(x\), and \(g\) is the link function. The exponential family of distributions can be expressed as:

\begin{align*}
    f_\theta(y) = e^{\eta(\theta)T(y) - B(\theta)} h(y)
\end{align*}

This family includes various common distributions such as Normal, Exponential, Poisson, and Bernoulli. Additionally, the canonical exponential family is represented as:

\begin{align*}
    f_\theta(y) = e^{\frac{y\theta - b(\theta)}{\phi} + c(y, \phi)}
\end{align*}

In this formulation, \(b'(\theta) = \mu\), where \(\mu\) is the expected value conditioned on \(x\). This allows us to derive the parameter from the conditioned expected value. The log-likelihood, using the canonical link function, becomes:

\begin{align*}
    l_n(Y, X, \beta) = \sum_i \frac{Y_i X_i^T \beta - b(X_i^T \beta)}{\phi}
\end{align*}

This log-likelihood function is strictly concave when \(\phi > 0\), ensuring a unique MLE. When using other parametrizations, multiple local maxima may exist. The asymptotic normality of MLE is also applicable to GLMs.

\end{block}


\end{column}
\end{columns}
\end{document}